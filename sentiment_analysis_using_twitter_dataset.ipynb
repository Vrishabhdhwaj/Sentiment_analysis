{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-13T16:36:45.875571Z","iopub.execute_input":"2021-11-13T16:36:45.876461Z","iopub.status.idle":"2021-11-13T16:36:45.907367Z","shell.execute_reply.started":"2021-11-13T16:36:45.876361Z","shell.execute_reply":"2021-11-13T16:36:45.906555Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Importing the necessary packages and modules","metadata":{}},{"cell_type":"code","source":"import re # for using regular expression (regex)\nimport pandas as pd # reading and processing the dataset\nimport matplotlib.pyplot as plt # for graphs and visualization\nimport seaborn as sns # plotting heatmaps\nimport pickle # to save the model weights for future use\nimport nltk # natural language toolkit library for text processing\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer # lemmatization package\nfrom sklearn.feature_extraction.text import TfidfVectorizer # vectorizer","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:36:49.088599Z","iopub.execute_input":"2021-11-13T16:36:49.089053Z","iopub.status.idle":"2021-11-13T16:36:50.890308Z","shell.execute_reply.started":"2021-11-13T16:36:49.089010Z","shell.execute_reply":"2021-11-13T16:36:50.889431Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Reading the dataset","metadata":{}},{"cell_type":"code","source":"# defining the column names of the dataframe\ncolumns = ['target','id','date','flag','user','text']\n# 'target' refers to the sentiment of the tweet (0 means negative, 2 means neutral and 4 means positive)\n# 'text is the tweet by the user'\ndf = pd.read_csv('/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv',encoding='ISO-8859-1',names=columns)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:38:13.428135Z","iopub.execute_input":"2021-11-13T16:38:13.428557Z","iopub.status.idle":"2021-11-13T16:38:17.800479Z","shell.execute_reply.started":"2021-11-13T16:38:13.428529Z","shell.execute_reply":"2021-11-13T16:38:17.799505Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Dataset description and selecting the useful data","metadata":{}},{"cell_type":"code","source":"df.head(100)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:38:28.912952Z","iopub.execute_input":"2021-11-13T16:38:28.913245Z","iopub.status.idle":"2021-11-13T16:38:28.928640Z","shell.execute_reply.started":"2021-11-13T16:38:28.913213Z","shell.execute_reply":"2021-11-13T16:38:28.928078Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:38:43.945558Z","iopub.execute_input":"2021-11-13T16:38:43.946609Z","iopub.status.idle":"2021-11-13T16:38:43.952152Z","shell.execute_reply.started":"2021-11-13T16:38:43.946572Z","shell.execute_reply":"2021-11-13T16:38:43.951310Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df.isnull().any()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:38:51.401336Z","iopub.execute_input":"2021-11-13T16:38:51.401966Z","iopub.status.idle":"2021-11-13T16:38:52.098402Z","shell.execute_reply.started":"2021-11-13T16:38:51.401921Z","shell.execute_reply":"2021-11-13T16:38:52.097807Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# selecting only the target and text columns from the dataframe\ndf = df[['target','text']]\ndf","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:39:38.208522Z","iopub.execute_input":"2021-11-13T16:39:38.208930Z","iopub.status.idle":"2021-11-13T16:39:38.245857Z","shell.execute_reply.started":"2021-11-13T16:39:38.208902Z","shell.execute_reply":"2021-11-13T16:39:38.245304Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:39:42.350306Z","iopub.execute_input":"2021-11-13T16:39:42.350711Z","iopub.status.idle":"2021-11-13T16:39:42.365490Z","shell.execute_reply.started":"2021-11-13T16:39:42.350683Z","shell.execute_reply":"2021-11-13T16:39:42.364704Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# As it turns out the dataset does not have any neutral sentiment tweets, so now we can consider this as a binary classification problem","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:39:45.214570Z","iopub.execute_input":"2021-11-13T16:39:45.214859Z","iopub.status.idle":"2021-11-13T16:39:45.218843Z","shell.execute_reply.started":"2021-11-13T16:39:45.214829Z","shell.execute_reply":"2021-11-13T16:39:45.218034Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# plotting the counts of sentiments for each class \nsns.countplot(data=df,x='target')\nplt.xticks(ticks=[0,1],labels=['Negative','Positive'])\nplt.xlabel('Sentiment')\nplt.ylabel('Value count of the sentiments')","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:41:41.788780Z","iopub.execute_input":"2021-11-13T16:41:41.789089Z","iopub.status.idle":"2021-11-13T16:41:42.088078Z","shell.execute_reply.started":"2021-11-13T16:41:41.789057Z","shell.execute_reply":"2021-11-13T16:41:42.087123Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# replacing the class notation of positive statements from 4 to 1\ndf['target'] = df['target'].replace(4,1)\ndf['target'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:42:03.266050Z","iopub.execute_input":"2021-11-13T16:42:03.266347Z","iopub.status.idle":"2021-11-13T16:42:03.294293Z","shell.execute_reply.started":"2021-11-13T16:42:03.266314Z","shell.execute_reply":"2021-11-13T16:42:03.293447Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# to make use of emojis used in the dataset, we define a dictionary that maps emojis to their textual meaning.\nemojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad',':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed',':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused',\n          '$_$': 'greedy','@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused','<(-_-)>': 'robot', 'd[-_-]b': 'dj', \n          \":'-)\": 'sadsmile',';)': 'wink',';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n# Stopwords are those words that do not contribute much to the sentiment/meaning o fthe text.\n# These can be removed using Tfidf vectorization techniques.\nstopword =  ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an','and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do','does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here','hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma','me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them','themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was', 'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\"youve\", 'your', 'yours', 'yourself', 'yourselves']","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:42:13.562070Z","iopub.execute_input":"2021-11-13T16:42:13.563102Z","iopub.status.idle":"2021-11-13T16:42:13.576070Z","shell.execute_reply.started":"2021-11-13T16:42:13.563055Z","shell.execute_reply":"2021-11-13T16:42:13.574943Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Preprocessing function to clean the data.\ndef preprocess(text , wordLemm):\n    processedText = []\n    \n    # Regex patterns for handling urls, usernames and word patterns that do not contribute much to model training.\n    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n    userPattern       = '@[^\\s]+'\n    alphaPattern      = \"[^a-zA-Z0-9]\"\n    sequencePattern   = r\"(.)\\1\\1+\"\n    seqReplacePattern = r\"\\1\\1\"\n    \n    for tweet in text:\n        tweet = tweet.lower()\n        \n        # Replace all URls with 'URL'\n        tweet = re.sub(urlPattern,' URL',tweet)\n        # Replace all emojis.\n        for emoji in emojis.keys():\n            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n        # Replace @USERNAME to 'USER'.\n        tweet = re.sub(userPattern,' USER', tweet)        \n        # Replace all non alphabets.\n        tweet = re.sub(alphaPattern, \" \", tweet)\n        # Replace 3 or more consecutive letters by 2 letter.\n        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n        \n        tweetwords = ''\n        for word in tweet.split():\n            if len(word)>1:\n                # Lemmatizing the word.\n                word = wordLemm.lemmatize(word)\n                tweetwords += (word+' ')\n            \n        processedText.append(tweetwords)\n        \n    return processedText","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:42:22.175363Z","iopub.execute_input":"2021-11-13T16:42:22.177063Z","iopub.status.idle":"2021-11-13T16:42:22.184955Z","shell.execute_reply.started":"2021-11-13T16:42:22.177015Z","shell.execute_reply":"2021-11-13T16:42:22.184362Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Instantiate the `WordNetLemmatizer` and `TfidfVectorizer`","metadata":{}},{"cell_type":"code","source":"wordLemm = WordNetLemmatizer()\nX = preprocess(list(df['text']),wordLemm)\ny = df['target']\n\nvect = TfidfVectorizer(ngram_range=(1,2), max_features=1000000,stop_words=stopword)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:43:23.970530Z","iopub.execute_input":"2021-11-13T16:43:23.970808Z","iopub.status.idle":"2021-11-13T16:46:23.520956Z","shell.execute_reply.started":"2021-11-13T16:43:23.970776Z","shell.execute_reply":"2021-11-13T16:46:23.519675Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the vectorized data to train and test sets","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=42)\npd.DataFrame(X_train , y_train).info","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:46:23.523772Z","iopub.execute_input":"2021-11-13T16:46:23.524081Z","iopub.status.idle":"2021-11-13T16:46:24.705569Z","shell.execute_reply.started":"2021-11-13T16:46:23.524036Z","shell.execute_reply":"2021-11-13T16:46:24.704803Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"vect.fit(X_train)\n\nX_train = vect.transform(X_train)\nX_test = vect.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:46:24.706623Z","iopub.execute_input":"2021-11-13T16:46:24.706820Z","iopub.status.idle":"2021-11-13T16:48:23.411824Z","shell.execute_reply.started":"2021-11-13T16:46:24.706798Z","shell.execute_reply":"2021-11-13T16:48:23.411108Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Using Logistic Regression and Multinomial Naive Bayes ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Model performance parameters\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:50:24.556334Z","iopub.execute_input":"2021-11-13T16:50:24.556666Z","iopub.status.idle":"2021-11-13T16:50:24.564735Z","shell.execute_reply.started":"2021-11-13T16:50:24.556630Z","shell.execute_reply":"2021-11-13T16:50:24.563857Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### Multinomial Naive Bayes :","metadata":{}},{"cell_type":"code","source":"nb_clf = MultinomialNB()\nnb_clf.fit(X_train,y_train)\n\nnb_pred = nb_clf.predict(X_test)\nprint('Accuracy of Multinomial Naive Bayes:',accuracy_score(y_test,nb_pred))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:50:36.085625Z","iopub.execute_input":"2021-11-13T16:50:36.086440Z","iopub.status.idle":"2021-11-13T16:50:36.761866Z","shell.execute_reply.started":"2021-11-13T16:50:36.086402Z","shell.execute_reply":"2021-11-13T16:50:36.761080Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression :","metadata":{}},{"cell_type":"code","source":"log_clf = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1, penalty = 'l2', solver = 'newton-cg')\nlog_clf.fit(X_train,y_train)\n\nlog_pred = log_clf.predict(X_test)\nprint('Accuracy of Logistic Regression:',accuracy_score(y_test,log_pred))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:08:45.620389Z","iopub.execute_input":"2021-11-13T17:08:45.621267Z","iopub.status.idle":"2021-11-13T17:10:17.564678Z","shell.execute_reply.started":"2021-11-13T17:08:45.621221Z","shell.execute_reply":"2021-11-13T17:10:17.563773Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# example of grid searching key hyperparametres for logistic regression\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n# define dataset\nX, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n# define models and parameters\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-13T06:35:08.524179Z","iopub.execute_input":"2021-11-13T06:35:08.524847Z","iopub.status.idle":"2021-11-13T13:49:39.337568Z","shell.execute_reply.started":"2021-11-13T06:35:08.524811Z","shell.execute_reply":"2021-11-13T13:49:39.334954Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def tweet_analysis(str):\n    inp = [str]\n    print(log_clf.predict(vect.transform(inp)))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:11:23.572367Z","iopub.execute_input":"2021-11-13T17:11:23.572669Z","iopub.status.idle":"2021-11-13T17:11:23.577356Z","shell.execute_reply.started":"2021-11-13T17:11:23.572636Z","shell.execute_reply":"2021-11-13T17:11:23.576467Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"tweet_analysis(\"sentiment ananlysis is good :)\")","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:16:12.112929Z","iopub.execute_input":"2021-11-13T17:16:12.113698Z","iopub.status.idle":"2021-11-13T17:16:12.129716Z","shell.execute_reply.started":"2021-11-13T17:16:12.113661Z","shell.execute_reply":"2021-11-13T17:16:12.128757Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"**Logistic regression** is more accurate than others ","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test,log_pred)\nprint(cm)\n\nplt.figure(figsize=(5,5))\nsns.heatmap(cm,annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:16:20.517794Z","iopub.execute_input":"2021-11-13T17:16:20.518098Z","iopub.status.idle":"2021-11-13T17:16:21.215819Z","shell.execute_reply.started":"2021-11-13T17:16:20.518061Z","shell.execute_reply":"2021-11-13T17:16:21.215009Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,log_pred))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T17:16:39.428588Z","iopub.execute_input":"2021-11-13T17:16:39.429275Z","iopub.status.idle":"2021-11-13T17:16:39.968094Z","shell.execute_reply.started":"2021-11-13T17:16:39.429227Z","shell.execute_reply":"2021-11-13T17:16:39.967146Z"},"trusted":true},"execution_count":43,"outputs":[]}]}